# -*- coding: utf-8 -*-
import os, time, random, json, subprocess
import pandas as pd
import yfinance as yf
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ========== åƒæ•¸èˆ‡è·¯å¾‘ ==========
MARKET_CODE = "cn-share"
DATA_SUBDIR = "dayK"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data", MARKET_CODE, DATA_SUBDIR)
# å¿«å–è·¯å¾‘ï¼šæ”¾åœ¨ data/cn-share/lists ä¸‹ä»¥ä¿æŒçµæ§‹æ•´æ½”
LIST_DIR = os.path.join(BASE_DIR, "data", MARKET_CODE, "lists")
CACHE_LIST_PATH = os.path.join(LIST_DIR, "cn_stock_list_cache.json")

THREADS_CN = 4
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(LIST_DIR, exist_ok=True)

def log(msg: str):
    print(f"{pd.Timestamp.now():%H:%M:%S}: {msg}")

def get_cn_list():
    """ä½¿ç”¨ akshare ç²å– A è‚¡æ¸…å–®ï¼Œå…·å‚™å¤šé‡é˜²å‘†èˆ‡é‡è©¦æ©Ÿåˆ¶"""
    threshold = 4000  # æ­£å¸¸ A è‚¡æ‡‰æœ‰ 5000 æª”ä»¥ä¸Šï¼Œä½æ–¼ 4000 è¦–ç‚ºç•°å¸¸
    
    # 1. æª¢æŸ¥ä»Šæ—¥å¿«å–
    if os.path.exists(CACHE_LIST_PATH):
        try:
            file_mtime = os.path.getmtime(CACHE_LIST_PATH)
            if datetime.fromtimestamp(file_mtime).date() == datetime.now().date():
                with open(CACHE_LIST_PATH, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if len(data) >= threshold:
                        log(f"ğŸ“¦ è¼‰å…¥ä»Šæ—¥ A è‚¡æ¸…å–®å¿«å– (å…± {len(data)} æª”)...")
                        return data
        except Exception as e:
            log(f"âš ï¸ è®€å–å¿«å–å¤±æ•—: {e}")

    # 2. é€²å…¥ API é‡è©¦è¿´åœˆ
    import akshare as ak
    max_retries = 3
    for i in range(max_retries):
        log(f"ğŸ“¡ æ­£åœ¨å¾ akshare ç²å–æ¸…å–® (ç¬¬ {i+1}/{max_retries} æ¬¡å˜—è©¦)...")
        try:
            # ç²å– A è‚¡ä»£ç¢¼åç¨±
            df = ak.stock_info_a_code_name()
            
            # ä»£ç¢¼è£œé½Šèˆ‡éæ¿¾ (ç¢ºä¿ 000001 é€™ç¨®æ ¼å¼)
            df['code'] = df['code'].astype(str).str.zfill(6)
            valid_prefixes = ('000','001','002','003','300','301','302','600','601','603','605','688','689')
            df = df[df['code'].str.startswith(valid_prefixes)]
            
            res = [f"{row['code']}&{row['name']}" for _, row in df.iterrows()]
            
            # æ•¸é‡é–€æª»åˆ¤å®š
            if len(res) >= threshold:
                log(f"âœ… æˆåŠŸç²å– A è‚¡æ¸…å–®ï¼Œå…± {len(res)} æª”ã€‚")
                with open(CACHE_LIST_PATH, "w", encoding="utf-8") as f:
                    json.dump(res, f, ensure_ascii=False)
                return res
            else:
                log(f"âš ï¸ æ•¸é‡ç•°å¸¸ ({len(res)} æª”)ï¼Œæº–å‚™é‡è©¦...")
                
        except Exception as e:
            log(f"âŒ å˜—è©¦å¤±æ•—: {e}")
        
        if i < max_retries - 1:
            wait_time = (i + 1) * 5
            log(f"â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
            time.sleep(wait_time)

    # 3. çµ‚æ¥µä¿åº•ï¼šè®€å–æ­·å²å¿«å–
    if os.path.exists(CACHE_LIST_PATH):
        log("ğŸ”„ API è«‹æ±‚å…¨æ•¸å¤±æ•—ï¼Œä½¿ç”¨æ­·å²å¿«å–å‚™æ´...")
        with open(CACHE_LIST_PATH, "r", encoding="utf-8") as f:
            return json.load(f)

    log("ğŸš¨ è­¦å‘Šï¼šå®Œå…¨ç„¡æ³•å–å¾—æ¸…å–®ï¼ŒåŸ·è¡Œæœ€å°æ¸¬è©¦é›†ã€‚")
    return ["600519&è²´å·èŒ…å°", "000001&å¹³å®‰éŠ€è¡Œ"]

def download_one(item):
    """å–®æª”ä¸‹è¼‰é‚è¼¯"""
    try:
        code, name = item.split('&', 1)
        # Yahoo Finance æ ¼å¼ï¼š6 é–‹é ­ç‚ºä¸Šæµ· (.SS)ï¼Œå…¶é¤˜ç‚ºæ·±åœ³ (.SZ)
        symbol = f"{code}.SS" if code.startswith('6') else f"{code}.SZ"
        # æª”åæ ¼å¼ï¼šCode_Name.csv
        out_path = os.path.join(DATA_DIR, f"{code}_{name}.csv")

        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰å…§å®¹ (çºŒè·‘æ©Ÿåˆ¶)
        if os.path.exists(out_path) and os.path.getsize(out_path) > 1000:
            return {"status": "exists", "code": code}

        # é©åº¦æš«åœé¿å…è¢«é™æµ
        time.sleep(random.uniform(0.3, 0.8))
        
        tk = yf.Ticker(symbol)
        # æŠ“å– 2 å¹´æ—¥ K
        hist = tk.history(period="2y", timeout=20)
        
        if hist is not None and not hist.empty:
            hist.reset_index(inplace=True)
            hist.columns = [c.lower() for c in hist.columns]
            # ç¢ºä¿æ—¥æœŸæ²’æœ‰æ™‚å€å•é¡Œ
            if 'date' in hist.columns:
                hist['date'] = pd.to_datetime(hist['date'], utc=True).dt.tz_localize(None)
            
            hist.to_csv(out_path, index=False, encoding='utf-8-sig')
            return {"status": "success", "code": code}
        return {"status": "empty", "code": code}
    except Exception:
        return {"status": "error", "code": item.split('&')[0]}

def main():
    log("ğŸ‡¨ğŸ‡³ ä¸­åœ‹ A è‚¡ K ç·šä¸‹è¼‰å™¨å•Ÿå‹•")
    items = get_cn_list()
    log(f"ğŸš€ é–‹å§‹ä¸‹è¼‰è™•ç† (å…± {len(items)} æª”æ¨™çš„)")
    
    stats = {"success": 0, "exists": 0, "empty": 0, "error": 0}
    
    # ä½¿ç”¨å¤šåŸ·è¡Œç·’æå‡ä¸‹è¼‰é€Ÿåº¦
    with ThreadPoolExecutor(max_workers=THREADS_CN) as executor:
        futs = {executor.submit(download_one, it): it for it in items}
        pbar = tqdm(total=len(items), desc="CN ä¸‹è¼‰é€²åº¦")
        
        for f in as_completed(futs):
            res = f.result()
            stats[res.get("status", "error")] += 1
            pbar.update(1)
        
        pbar.close()
    
    log(f"ğŸ“Š A è‚¡ä¸‹è¼‰å ±å‘Š: æˆåŠŸ={stats['success']}, è·³é(å·²å­˜åœ¨)={stats['exists']}, å¤±æ•—={stats['error']}")

if __name__ == "__main__":
    main()
